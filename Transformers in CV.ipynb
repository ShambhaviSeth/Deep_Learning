{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eR3X8KNXVkGj"
      },
      "source": [
        "\n",
        "# Transformers in Computer Vision\n",
        "\n",
        "\n",
        "\n",
        "Transformer architectures owe their origins in natural language processing (NLP), and indeed form the core of the current state of the art models for most NLP applications.\n",
        "\n",
        "We will now see how to develop transformers for processing image data (and in fact, this line of deep learning research has been gaining a lot of attention in 2021). The *Vision Transformer* (ViT) introduced in [this paper](https://arxiv.org/pdf/2010.11929.pdf) shows how standard transformer architectures can perform very well on image. The high level idea is to extract patches from images, treat them as tokens, and pass them through a sequence of transformer blocks before throwing on a couple of dense classification layers at the very end.\n",
        "\n",
        "\n",
        "Some caveats to keep in mind:\n",
        "- ViT models are very cumbersome to train (since they involve a ton of parameters) so budget accordingly.\n",
        "- ViT models are a bit hard to interpret (even more so than regular convnets).\n",
        "- Finally, while in this notebook we will train a transformer from scratch, ViT models in practice are almost always *pre-trained* on some large dataset (such as ImageNet) before being transferred onto specific training datasets.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1m4W8BCYCG_"
      },
      "source": [
        "# Setup\n",
        "\n",
        "As usual, we start with basic data loading and preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcX3S4xjEoGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72431bae-58d8-4c63-a0de-dc843ab0e91b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gk0KHIjUDRSH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tL2k3L-mDkDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12e4fd02-22e1-4378-98c1-847b14b1e221"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 9508524.53it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 164932.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3156089.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 22848970.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./FashionMNIST/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# defining path for the dataset and batch size for training and testing data\n",
        "DOWNLOAD_PATH = '/data/FashionMNIST'\n",
        "BATCH_SIZE_TRAIN = 256\n",
        "BATCH_SIZE_TEST = 1000\n",
        "\n",
        "# Defining the transformation that needs to be done on the dataset\n",
        "transform_mnist = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "# Loading the training and testing data\n",
        "trainingdata = torchvision.datasets.FashionMNIST('./FashionMNIST/',train=True,download=True,transform=transform_mnist)\n",
        "testdata = torchvision.datasets.FashionMNIST('./FashionMNIST/',train=False,download=True,transform=transform_mnist)\n",
        "\n",
        "trainDataLoader = torch.utils.data.DataLoader(trainingdata,batch_size=BATCH_SIZE_TRAIN,shuffle=True)\n",
        "testDataLoader = torch.utils.data.DataLoader(testdata,batch_size=BATCH_SIZE_TEST,shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3Zi6xFSbU5M"
      },
      "source": [
        "# The ViT Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNkMNRs4c4Ja"
      },
      "source": [
        "We will now set up the ViT model. There will be 3 parts to this model:\n",
        "\n",
        "* A ``patch embedding'' layer that takes an image and tokenizes it. There is some amount of tensor algebra involved here (since we have to slice and dice the input appropriately), and the `einops` package is helpful. We will also add learnable positional encodings as parameters.\n",
        "* A sequence of transformer blocks. This will be a smaller scale replica of the original proposed ViT, except that we will only use 4 blocks in our model (instead of 32 in the actual ViT).\n",
        "* A (dense) classification layer at the end.\n",
        "\n",
        "Further, each transformer block consists of the following components:\n",
        "\n",
        "* A *self-attention* layer with $H$ heads,\n",
        "* A one-hidden-layer (dense) network to collapse the various heads. For the hidden neurons, the original ViT used something called a [GeLU](https://arxiv.org/pdf/1606.08415.pdf) activation function, which is a smooth approximation to the ReLU. For our example, regular ReLUs seem to be working just fine. The original ViT also used Dropout but we won't need it here.\n",
        "* *layer normalization* preceeding each of the above operations.\n",
        "\n",
        "Some care needs to be taken in making sure the various dimensions of the tensors are matched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAuZ2Nm_DXCB"
      },
      "outputs": [],
      "source": [
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)\n",
        "\n",
        "# classes\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.ReLU(), #nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
        "\n",
        "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "\n",
        "        attn = self.attend(dots)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
        "            ]))\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.mlp_head(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vD4p1EyWHbD"
      },
      "outputs": [],
      "source": [
        "# Defining the model with the mentioned hyperparameters\n",
        "model = ViT(image_size=28, patch_size=4, num_classes=10, channels=1, dim=64, depth=6, heads=4, mlp_dim=128)\n",
        "\n",
        "# Initialising the optimiser for the model\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK7gfKWm0ggw"
      },
      "source": [
        "Checking what the model looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riukWYK5WIkB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e13effd4-507c-4f92-82cb-14127159a84d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViT(\n",
              "  (to_patch_embedding): Sequential(\n",
              "    (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=4, p2=4)\n",
              "    (1): Linear(in_features=16, out_features=64, bias=True)\n",
              "  )\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              "  (transformer): Transformer(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x ModuleList(\n",
              "        (0): PreNorm(\n",
              "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (to_qkv): Linear(in_features=64, out_features=768, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=256, out_features=64, bias=True)\n",
              "              (1): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Dropout(p=0.0, inplace=False)\n",
              "              (3): Linear(in_features=128, out_features=64, bias=True)\n",
              "              (4): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (to_latent): Identity()\n",
              "  (mlp_head): Sequential(\n",
              "    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (1): Linear(in_features=64, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo5XbEtJbtiG"
      },
      "source": [
        "The model has 4 transformer blocks, followed by a linear classification layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTN9-kFMbdXn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcb062a8-afe7-4ce2-cb5c-c336f21ce88a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "499722\n"
          ]
        }
      ],
      "source": [
        "# Calculating the number of trainable parameters for which requires_grad = True\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(count_parameters(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgSVq3NqcKT3"
      },
      "source": [
        "The model defined above has almost about half a million trainable parameters. And since we are training on FashionMNIST this should be more than sufficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mnUt-GXYYv3"
      },
      "source": [
        "# Training and testing\n",
        "\n",
        "The below code is for training and evaluating the model on the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXxnADQNDpgA"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, optimizer, data_loader, loss_history):\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    model.train()\n",
        "\n",
        "    for i, (data, target) in enumerate(data_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = F.log_softmax(model(data), dim=1)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +\n",
        "                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)]  Loss: ' +\n",
        "                  '{:6.4f}'.format(loss.item()))\n",
        "            loss_history.append(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Moywc015DrAg"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, loss_history):\n",
        "    model.eval()\n",
        "\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    correct_samples = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            output = F.log_softmax(model(data), dim=1)\n",
        "            loss = F.nll_loss(output, target, reduction='sum')\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct_samples += pred.eq(target).sum()\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    loss_history.append(avg_loss)\n",
        "    print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n",
        "          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n",
        "          '{:5}'.format(total_samples) + ' (' +\n",
        "          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d0Hb-TKDwbA",
        "outputId": "54072b31-b133-4077-feb6-d33aee780d43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "[    0/60000 (  0%)]  Loss: 2.3579\n",
            "[25600/60000 ( 43%)]  Loss: 0.5458\n",
            "[51200/60000 ( 85%)]  Loss: 0.4557\n",
            "\n",
            "Average test loss: 0.4821  Accuracy: 8224/10000 (82.24%)\n",
            "\n",
            "Epoch: 2\n",
            "[    0/60000 (  0%)]  Loss: 0.3904\n",
            "[25600/60000 ( 43%)]  Loss: 0.5131\n",
            "[51200/60000 ( 85%)]  Loss: 0.2664\n",
            "\n",
            "Average test loss: 0.4334  Accuracy: 8402/10000 (84.02%)\n",
            "\n",
            "Epoch: 3\n",
            "[    0/60000 (  0%)]  Loss: 0.3704\n",
            "[25600/60000 ( 43%)]  Loss: 0.2838\n",
            "[51200/60000 ( 85%)]  Loss: 0.2319\n",
            "\n",
            "Average test loss: 0.3933  Accuracy: 8554/10000 (85.54%)\n",
            "\n",
            "Epoch: 4\n",
            "[    0/60000 (  0%)]  Loss: 0.3825\n",
            "[25600/60000 ( 43%)]  Loss: 0.3465\n",
            "[51200/60000 ( 85%)]  Loss: 0.3413\n",
            "\n",
            "Average test loss: 0.3826  Accuracy: 8589/10000 (85.89%)\n",
            "\n",
            "Epoch: 5\n",
            "[    0/60000 (  0%)]  Loss: 0.4065\n",
            "[25600/60000 ( 43%)]  Loss: 0.3423\n",
            "[51200/60000 ( 85%)]  Loss: 0.3196\n",
            "\n",
            "Average test loss: 0.3555  Accuracy: 8729/10000 (87.29%)\n",
            "\n",
            "Epoch: 6\n",
            "[    0/60000 (  0%)]  Loss: 0.3405\n",
            "[25600/60000 ( 43%)]  Loss: 0.3594\n",
            "[51200/60000 ( 85%)]  Loss: 0.3106\n",
            "\n",
            "Average test loss: 0.3437  Accuracy: 8744/10000 (87.44%)\n",
            "\n",
            "Epoch: 7\n",
            "[    0/60000 (  0%)]  Loss: 0.2486\n",
            "[25600/60000 ( 43%)]  Loss: 0.2999\n",
            "[51200/60000 ( 85%)]  Loss: 0.2740\n",
            "\n",
            "Average test loss: 0.3433  Accuracy: 8733/10000 (87.33%)\n",
            "\n",
            "Epoch: 8\n",
            "[    0/60000 (  0%)]  Loss: 0.2778\n",
            "[25600/60000 ( 43%)]  Loss: 0.2632\n",
            "[51200/60000 ( 85%)]  Loss: 0.2815\n",
            "\n",
            "Average test loss: 0.3397  Accuracy: 8776/10000 (87.76%)\n",
            "\n",
            "Epoch: 9\n",
            "[    0/60000 (  0%)]  Loss: 0.2245\n",
            "[25600/60000 ( 43%)]  Loss: 0.2457\n",
            "[51200/60000 ( 85%)]  Loss: 0.2477\n",
            "\n",
            "Average test loss: 0.3298  Accuracy: 8827/10000 (88.27%)\n",
            "\n",
            "Epoch: 10\n",
            "[    0/60000 (  0%)]  Loss: 0.2201\n",
            "[25600/60000 ( 43%)]  Loss: 0.3567\n",
            "[51200/60000 ( 85%)]  Loss: 0.2671\n",
            "\n",
            "Average test loss: 0.3223  Accuracy: 8826/10000 (88.26%)\n",
            "\n",
            "Epoch: 11\n",
            "[    0/60000 (  0%)]  Loss: 0.2442\n",
            "[25600/60000 ( 43%)]  Loss: 0.3017\n",
            "[51200/60000 ( 85%)]  Loss: 0.2008\n",
            "\n",
            "Average test loss: 0.3365  Accuracy: 8789/10000 (87.89%)\n",
            "\n",
            "Epoch: 12\n",
            "[    0/60000 (  0%)]  Loss: 0.2293\n",
            "[25600/60000 ( 43%)]  Loss: 0.2306\n",
            "[51200/60000 ( 85%)]  Loss: 0.2525\n",
            "\n",
            "Average test loss: 0.3200  Accuracy: 8837/10000 (88.37%)\n",
            "\n",
            "Epoch: 13\n",
            "[    0/60000 (  0%)]  Loss: 0.2339\n",
            "[25600/60000 ( 43%)]  Loss: 0.2223\n",
            "[51200/60000 ( 85%)]  Loss: 0.2465\n",
            "\n",
            "Average test loss: 0.3272  Accuracy: 8828/10000 (88.28%)\n",
            "\n",
            "Epoch: 14\n",
            "[    0/60000 (  0%)]  Loss: 0.2552\n",
            "[25600/60000 ( 43%)]  Loss: 0.2598\n",
            "[51200/60000 ( 85%)]  Loss: 0.2580\n",
            "\n",
            "Average test loss: 0.3372  Accuracy: 8796/10000 (87.96%)\n",
            "\n",
            "Epoch: 15\n",
            "[    0/60000 (  0%)]  Loss: 0.2164\n",
            "[25600/60000 ( 43%)]  Loss: 0.1918\n",
            "[51200/60000 ( 85%)]  Loss: 0.2734\n",
            "\n",
            "Average test loss: 0.3329  Accuracy: 8843/10000 (88.43%)\n",
            "\n",
            "Execution time: 4993.86 seconds\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 15\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Training the model for 15 epochs\n",
        "train_loss_history, test_loss_history = [], []\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('Epoch:', epoch)\n",
        "    train_epoch(model, optimizer, trainDataLoader, train_loss_history)\n",
        "    evaluate(model, testDataLoader, test_loss_history)\n",
        "\n",
        "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see accuracy increasing from 82% to 88%.\n"
      ],
      "metadata": {
        "id": "v12oYh3YWK4i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35jvT53FuKNM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1678d00-19fc-4840-dad2-25a5f43c9839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average test loss: 0.3329  Accuracy: 8843/10000 (88.43%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluating the model on the test dataset\n",
        "evaluate(model, testDataLoader, test_loss_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model has an accuracy of 88% which means it predicts correctly for most of the samples.\n"
      ],
      "metadata": {
        "id": "ym9t58UAV-Uw"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}